{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa2c5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a444ef32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: RunnableConfig | END | StateGraph | START\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ConfigSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, config: RunnableConfig):\\n    if config[\"configurable\"][\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif config[\"configurable\"][\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, config_schema=ConfigSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, {\"configurable\": {\"my_runtime_value\": \"a\"}}))\\nprint(graph.invoke({}, {\"configurable\": {\"my_runtime_value\": \"b\"}}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import MessagesState\\nfrom langgraph.graph import END, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\nclass ConfigSchema(TypedDict):\\n    model: str\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, config: RunnableConfig):\\n    model = config[\"configurable\"].get(\"model\", \"anthropic\")\\n    model = MODELS[model]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, config_schema=ConfigSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nconfig = {\"configurable\": {\"model\": \"openai\"}}\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, config=config)[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\nclass ConfigSchema(TypedDict):\\n    model: Optional[str]\\n    system_message: Optional[str]\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, config: RunnableConfig):\\n    model = config[\"configurable\"].get(\"model\", \"anthropic\")\\n    model = MODELS[model]\\n    messages = state[\"messages\"]\\n    if system_message := config[\"configurable\"].get(\"system_message\"):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, config_schema=ConfigSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nconfig = {\"configurable\": {\"model\": \"openai\", \"system_message\": \"Respond in Italian.\"}}\\nresponse = graph.invoke({\"messages\": [input_message]}, config)\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: list[str]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/why-langgraph\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35479a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: RunnableConfig | END | StateGraph | START\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ConfigSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, config: RunnableConfig):\\n    if config[\"configurable\"][\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif config[\"configurable\"][\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, config_schema=ConfigSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, {\"configurable\": {\"my_runtime_value\": \"a\"}}))\\nprint(graph.invoke({}, {\"configurable\": {\"my_runtime_value\": \"b\"}}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import MessagesState\\nfrom langgraph.graph import END, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\nclass ConfigSchema(TypedDict):\\n    model: str\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, config: RunnableConfig):\\n    model = config[\"configurable\"].get(\"model\", \"anthropic\")\\n    model = MODELS[model]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, config_schema=ConfigSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nconfig = {\"configurable\": {\"model\": \"openai\"}}\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, config=config)[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom typing_extensions import TypedDict\\n\\nclass ConfigSchema(TypedDict):\\n    model: Optional[str]\\n    system_message: Optional[str]\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, config: RunnableConfig):\\n    model = config[\"configurable\"].get(\"model\", \"anthropic\")\\n    model = MODELS[model]\\n    messages = state[\"messages\"]\\n    if system_message := config[\"configurable\"].get(\"system_message\"):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, config_schema=ConfigSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nconfig = {\"configurable\": {\"model\": \"openai\", \"system_message\": \"Respond in Italian.\"}}\\nresponse = graph.invoke({\"messages\": [input_message]}, config)\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: list[str]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [doc for sublist in docs for doc in sublist]\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b61229d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='Overview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "doc_splits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0cc40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add all this chunks to vector db\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "793595b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4cd00bb0-6d21-454f-8d4c-26a86e1a430b', metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.'),\n",
       " Document(id='d453824f-d6a8-4bf5-a1d8-81f9e75812ce', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='LangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph\",k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9e3df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever to retriever tool\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"retriever_vector_langgraph_blog\",\n",
    "    description='search and run information about langgraph'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de18fd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_langgraph_blog', description='search and run information about langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000209305363E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000209B2081E00>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000020930537740>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000209B2081E00>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8dd06a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n\\nVisualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph\\'s key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.invoke(\"what is langgraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86739e",
   "metadata": {},
   "source": [
    "### Langchain blogs - Seperate Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94d26100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Tutorials | 🦜️🔗 LangChain', 'description': 'New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTutorials | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsOn this pageTutorials\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\nGet started\\u200b\\nFamiliarize yourself with LangChain\\'s open-source components by building simple applications.\\nIf you\\'re looking to get started with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our supported integrations.\\n\\nChat models and prompts: Build a simple LLM application with prompt templates and chat models.\\nSemantic search: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\\nClassification: Classify text into categories or labels using chat models with structured outputs.\\nExtraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\\n\\nRefer to the how-to guides for more detail on using all LangChain components.\\nOrchestration\\u200b\\nGet started using LangGraph to assemble LangChain components into full-featured applications.\\n\\nChatbots: Build a chatbot that incorporates memory.\\nAgents: Build an agent that interacts with external tools.\\nRetrieval Augmented Generation (RAG) Part 1: Build an application that uses your own documents to inform its responses.\\nRetrieval Augmented Generation (RAG) Part 2: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\nQuestion-Answering with SQL: Build a question-answering system that executes SQL queries to inform its responses.\\nSummarization: Generate summaries of (potentially long) texts.\\nQuestion-Answering with Graph Databases: Build a question-answering system that queries a graph database to inform its responses.\\n\\nLangSmith\\u200b\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.\\nEvaluation\\u200b\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\nEvaluate your LLM application\\nEdit this pagePreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseGet startedOrchestrationLangSmithEvaluationCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | 🦜️🔗 LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Chatbot | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot\\nnoteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview\\u200b\\nWe\\'ll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\\n\\nPipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\\nSelect chat model:Google Gemini▾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage\\nAIMessage(content=\\'Your name is Bob! How can I help you today, Bob?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 14, \\'prompt_tokens\\': 33, \\'total_tokens\\': 47, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-34bcccb3-446e-42f2-b1de-52c09936c02c-0\\', usage_metadata={\\'input_tokens\\': 33, \\'output_tokens\\': 14, \\'total_tokens\\': 47, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot\\'s ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence\\u200b\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\nRight now, all we\\'ve done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates\\u200b\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let\\'s now make that a bit more complicated. First, let\\'s add in a system message with some custom instructions (but still taking messages as input). Next, we\\'ll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages\\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¿Hay algo más en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History\\u200b\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we\\'ll use the trim_messages helper to reduce how many messages we\\'re sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages\\n[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    trimmed_messages = trimmer.invoke(state[\"messages\"])    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history:\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. You haven\\'t told me yet!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem did I ask?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================You asked what 2 + 2 equals.\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming\\u200b\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|’s| a| joke| for| you|:|Why| don|’t| skeleton|s| fight| each| other|?|Because| they| don|’t| have| the| guts|!||\\nNext Steps\\u200b\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pagePreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 2 | 🦜️🔗 LangChain', 'description': 'In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 2 | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 2On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 2\\nIn many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\\nThis is the second part of a multi-part tutorial:\\n\\nPart 1 introduces RAG and walks through a minimal implementation.\\nPart 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nHere we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\\nWe will cover two approaches:\\n\\nChains, in which we execute at most one retrieval step;\\nAgents, in which we give an LLM discretion to execute multiple retrieval steps.\\n\\nnoteThe methods presented here leverage tool-calling capabilities in modern chat models. See this page for a table of models supporting tool calling features.\\nFor the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the Part 1 of the RAG tutorial.\\nSetup\\u200b\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:Google Gemini▾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\\n\\nSelect embeddings model:OpenAI▾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect vector store:In-memory▾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nDependencies\\u200b\\nIn addition, we\\'ll use the following packages:\\n%%capture --no-stderr%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"if not os.environ.get(\"LANGSMITH_API_KEY\"):    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nChains\\u200b\\nLet\\'s first revisit the vector store we built in Part 1, which indexes an LLM Powered Autonomous Agents blog post by Lilian Weng.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)API Reference:hub | WebBaseLoader | Document | RecursiveCharacterTextSplitter\\n# Index chunks_ = vector_store.add_documents(documents=all_splits)\\nIn the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\\n\\nUser input as a HumanMessage;\\nVector store query as an AIMessage with tool calls;\\nRetrieved documents as a ToolMessage;\\nFinal response as a AIMessage.\\n\\nThis model for state is so versatile that LangGraph offers a built-in version for convenience:\\nfrom langgraph.graph import MessagesState, StateGraphgraph_builder = StateGraph(MessagesState)API Reference:StateGraph\\nLeveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\\n\\nHuman: \"What is Task Decomposition?\"\\nAI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\\nHuman: \"What are common ways of doing it?\"\\n\\nIn this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).\\nLet\\'s turn our retrieval step into a tool:\\nfrom langchain_core.tools import tool@tool(response_format=\"content_and_artifact\")def retrieve(query: str):    \"\"\"Retrieve information related to a query.\"\"\"    retrieved_docs = vector_store.similarity_search(query, k=2)    serialized = \"\\\\n\\\\n\".join(        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")        for doc in retrieved_docs    )    return serialized, retrieved_docsAPI Reference:tool\\nSee this guide for more detail on creating tools.\\nOur graph will consist of three nodes:\\n\\nA node that fields the user input, either generating a query for the retriever or responding directly;\\nA node for the retriever tool that executes the retrieval step;\\nA node that generates the final response using the retrieved context.\\n\\nWe build them below. Note that we leverage another pre-built LangGraph component, ToolNode, that executes the tool and adds the result as a ToolMessage to the state.\\nfrom langchain_core.messages import SystemMessagefrom langgraph.prebuilt import ToolNode# Step 1: Generate an AIMessage that may include a tool-call to be sent.def query_or_respond(state: MessagesState):    \"\"\"Generate tool call for retrieval or respond.\"\"\"    llm_with_tools = llm.bind_tools([retrieve])    response = llm_with_tools.invoke(state[\"messages\"])    # MessagesState appends messages to state instead of overwriting    return {\"messages\": [response]}# Step 2: Execute the retrieval.tools = ToolNode([retrieve])# Step 3: Generate a response using the retrieved content.def generate(state: MessagesState):    \"\"\"Generate answer.\"\"\"    # Get generated ToolMessages    recent_tool_messages = []    for message in reversed(state[\"messages\"]):        if message.type == \"tool\":            recent_tool_messages.append(message)        else:            break    tool_messages = recent_tool_messages[::-1]    # Format into prompt    docs_content = \"\\\\n\\\\n\".join(doc.content for doc in tool_messages)    system_message_content = (        \"You are an assistant for question-answering tasks. \"        \"Use the following pieces of retrieved context to answer \"        \"the question. If you don\\'t know the answer, say that you \"        \"don\\'t know. Use three sentences maximum and keep the \"        \"answer concise.\"        \"\\\\n\\\\n\"        f\"{docs_content}\"    )    conversation_messages = [        message        for message in state[\"messages\"]        if message.type in (\"human\", \"system\")        or (message.type == \"ai\" and not message.tool_calls)    ]    prompt = [SystemMessage(system_message_content)] + conversation_messages    # Run    response = llm.invoke(prompt)    return {\"messages\": [response]}API Reference:SystemMessage | ToolNode\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the steps into a sequence. We also allow the first query_or_respond step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\\nfrom langgraph.graph import ENDfrom langgraph.prebuilt import ToolNode, tools_conditiongraph_builder.add_node(query_or_respond)graph_builder.add_node(tools)graph_builder.add_node(generate)graph_builder.set_entry_point(\"query_or_respond\")graph_builder.add_conditional_edges(    \"query_or_respond\",    tools_condition,    {END: END, \"tools\": \"tools\"},)graph_builder.add_edge(\"tools\", \"generate\")graph_builder.add_edge(\"generate\", END)graph = graph_builder.compile()API Reference:ToolNode | tools_condition\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nLet\\'s test our application.\\nNote that it responds appropriately to messages that do not require an additional retrieval step:\\ninput_message = \"Hello\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Hello==================================\\x1b[1m Ai Message \\x1b[0m==================================Hello! How can I assist you today?\\nAnd when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh) Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.\\nCheck out the LangSmith trace here.\\nStateful management of chat history\\u200b\\nnoteThis section of the tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nIn production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nTo manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nFor a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide.\\nfrom langgraph.checkpoint.memory import MemorySavermemory = MemorySaver()graph = graph_builder.compile(checkpointer=memory)# Specify an ID for the threadconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}API Reference:MemorySaver\\nWe can now invoke similar to before:\\ninput_message = \"What is Task Decomposition?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is Task Decomposition?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN) Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN  Args:    query: Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.==================================\\x1b[1m Ai Message \\x1b[0m==================================Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model\\'s reasoning and makes it easier to tackle difficult problems.\\ninput_message = \"Can you look up some common ways of doing it?\"for step in graph.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    step[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================Can you look up some common ways of doing it?==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux) Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux  Args:    query: common methods of task decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", (2) employing task-specific instructions such as \"Write a story outline\" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.\\nNote that the query generated by the model in the second question incorporates the conversational context.\\nThe LangSmith trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.\\nAgents\\u200b\\nAgents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\\nBelow we assemble a minimal RAG agent. Using LangGraph\\'s pre-built ReAct agent constructor, we can do this in one line.\\ntipCheck out LangGraph\\'s Agentic RAG tutorial for more advanced formulations.\\nfrom langgraph.prebuilt import create_react_agentagent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)API Reference:create_react_agent\\nLet\\'s inspect the graph:\\ndisplay(Image(agent_executor.get_graph().draw_mermaid_png()))\\n\\nThe key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.\\nLet\\'s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}input_message = (    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"    \"Once you get the answer, look up common extensions of that method.\")for event in agent_executor.stream(    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},    stream_mode=\"values\",    config=config,):    event[\"messages\"][-1].pretty_print()\\n================================\\x1b[1m Human Message \\x1b[0m=================================What is the standard method for Task Decomposition?Once you get the answer, look up common extensions of that method.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N) Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N  Args:    query: standard method for Task Decomposition=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================Tool Calls:  retrieve (call_2JntP1x4XQMWwgVpYurE12ff) Call ID: call_2JntP1x4XQMWwgVpYurE12ff  Args:    query: common extensions of Task Decomposition methods=================================\\x1b[1m Tool Message \\x1b[0m=================================Name: retrieveSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}Content: Fig. 1. Overview of a LLM-powered autonomous agent system.Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.==================================\\x1b[1m Ai Message \\x1b[0m==================================The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to \"think step by step\" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:1. Simple prompting (e.g., asking for steps to achieve a goal).2. Task-specific instructions (e.g., asking for a story outline).3. Human inputs to guide the decomposition process.### Common Extensions of Task Decomposition Methods:1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic conversational Q&A application:\\n\\nWe used chains to build a predictable application that generates at most one query per user input;\\nWe used agents to build an application that can iterate on a sequence of queries.\\n\\nTo explore different types of retrievers and retrieval strategies, visit the retrievers section of the how-to guides.\\nFor a detailed walkthrough of LangChain\\'s conversation memory abstractions, visit the How to add message history (memory) guide.\\nTo learn more about agents, check out the conceptual guide and LangGraph agent architectures page.Edit this pagePreviousBuild a ChatbotNextBuild an Extraction ChainSetupComponentsDependenciesLangSmithChainsStateful management of chat historyAgentsNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls = [\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "891fa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "## add all these text to vectordb\n",
    "\n",
    "vectorstorelangchain = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    ")\n",
    "\n",
    "retrieverlangchain = vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35bc8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain = create_retriever_tool(\n",
    "    retriever=retrieverlangchain,\n",
    "    name='retriever_vector_langchain_blog',\n",
    "    description='search and run information about langchain'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1723765",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e1ab1",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "004e7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage,AIMessage, SystemMessage, BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # add_message function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says 'append'\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6789c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4cc9ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state.\n",
    "    Given the question, it will decide to retrieve using retriever tool, or simply end.\n",
    "    Args:\n",
    "        state (messages): the current state\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages \n",
    "    \"\"\"\n",
    "    print('--------Call Agent---------')\n",
    "    messages = state['messages']\n",
    "    model = llm.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # we return a list, because this will get added to the existing list\n",
    "    return {'messages':[response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5adf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal['generate', 'rewrite']:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): the current state\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"-------Check relevance--------\")\n",
    "    # data model\n",
    "    \n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "        binary_score: str = Field(description=\"relevance score 'yes' or 'no' \")\n",
    "    \n",
    "    # llm with tool and validation\n",
    "    llm_with_tool = llm.with_structured_output(grade)\n",
    "\n",
    "    # prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "                you are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "                Here is the question: {question} \\n\\n\n",
    "                Here is the retrieved documents: {context} \\n\\n\n",
    "                If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "                Given a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "                \"\"\",\n",
    "                input_variables=['context','question']\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    \n",
    "    scored_result = chain.invoke({\"question\":question,\"context\":docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"--------Decision: Docs relevant--------\")\n",
    "        return 'generate'\n",
    "    else:\n",
    "        print(\"--------Decision: Docs not relevant--------\")\n",
    "        return 'rewrite'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0e0b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    Args:\n",
    "        state (messages): the current state\n",
    "    Returns:\n",
    "        dict: the updated message \n",
    "    \"\"\"\n",
    "    print(\"--------Generate----------\")\n",
    "    messages = state['messages']\n",
    "    question, docs = messages[0].content, messages[-1].content\n",
    "\n",
    "    #prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # post processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    #run\n",
    "    response = rag_chain.invoke({\n",
    "        'context':docs,\n",
    "        \"question\": question\n",
    "    })\n",
    "    return {'messages':[response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2da92d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to  produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state(messages): the current state\n",
    "    Returns:\n",
    "        dict: the updated state re-phrased question \n",
    "    \"\"\"\n",
    "    print(\"-------Transform query---------\")\n",
    "    messages = state['messages']\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "                Look at the input and try to reason about underlying semantic intent / meaning. \\n \n",
    "                Here is the initial question:\n",
    "                \\n ----------\\n \n",
    "                {question}\n",
    "                \\n ----------\\n\n",
    "                Formulate an improved question:\n",
    "                    \"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # grader\n",
    "    response = llm.invoke(msg)\n",
    "    return {'messages': [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "625079af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END , StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display \n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node('agent',agent)\n",
    "retrieve = ToolNode(tools)\n",
    "workflow.add_node('retrieve',retrieve)\n",
    "workflow.add_node('rewrite',rewrite)\n",
    "workflow.add_node('generate',generate)\n",
    "workflow.add_edge(START,'agent')\n",
    "\n",
    "#decide whether to retrieve \n",
    "workflow.add_conditional_edges(\n",
    "    'agent',\n",
    "    tools_condition,\n",
    "    {\n",
    "        'tools':'retrieve',\n",
    "        END:END\n",
    "    }\n",
    ")\n",
    "#edge taken after the 'agent' node is called\n",
    "workflow.add_conditional_edges(\n",
    "    'retrieve',\n",
    "    grade_documents\n",
    ")\n",
    "workflow.add_edge('generate',END)\n",
    "workflow.add_edge('rewrite','agent')\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ec6b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Call Agent---------\n",
      "-------Check relevance--------\n",
      "--------Decision: Docs relevant--------\n",
      "--------Generate----------\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke(\n",
    "    {'messages': \"what is langchain?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb52473c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is langchain?', additional_kwargs={}, response_metadata={}, id='16b3a48f-ad73-499c-80bd-27a6ef36d716'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '549j6vgkn', 'function': {'arguments': '{\"query\":\"what is langchain\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 312, 'total_tokens': 334, 'completion_time': 0.029333333, 'prompt_time': 0.018414637, 'queue_time': 0.050375947000000004, 'total_time': 0.04774797}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f09ea464-cec1-47e0-a79d-cb6f41d44589-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'what is langchain'}, 'id': '549j6vgkn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 312, 'output_tokens': 22, 'total_tokens': 334}),\n",
       "  ToolMessage(content='LangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n\\nCopyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model', name='retriever_vector_langchain_blog', id='a5c19319-5799-4a01-93a1-752bff75511f', tool_call_id='549j6vgkn'),\n",
       "  HumanMessage(content='LangChain is a library that enables the creation of ambient agents. It provides a persistence layer called LangGraph, which supports features such as human-in-the-loop, memory, streaming, and deployment.', additional_kwargs={}, response_metadata={}, id='55a5929b-43ea-4657-b013-fa3d9f7ee67b')]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08668d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is langchain?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever_vector_langchain_blog (549j6vgkn)\n",
      " Call ID: 549j6vgkn\n",
      "  Args:\n",
      "    query: what is langchain\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_vector_langchain_blog\n",
      "\n",
      "LangSmith Trace\n",
      "https://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\n",
      "What LangGraph provides¶\n",
      "By constructing each of the above in LangGraph, we get a few things:\n",
      "Persistence: Human-in-the-Loop¶\n",
      "LangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\n",
      "Persistence: Memory¶\n",
      "LangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\n",
      "Streaming¶\n",
      "LangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\n",
      "Deployment¶\n",
      "LangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Previous\n",
      "              \n",
      "\n",
      "                Run a local server\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Next\n",
      "              \n",
      "\n",
      "                Agent architectures\n",
      "\n",
      "Overview\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Skip to content\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "            \n",
      "Our Building Ambient Agents with LangGraph course is now available on LangChain Academy!\n",
      "\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            LangGraph\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "            \n",
      "              Overview\n",
      "            \n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Initializing search\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    GitHub\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "  \n",
      "    \n",
      "  \n",
      "  Get started\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "  \n",
      "    \n",
      "  \n",
      "  Guides\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "  \n",
      "    \n",
      "  \n",
      "  Reference\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "  \n",
      "    \n",
      "  \n",
      "  Examples\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "          \n",
      "  \n",
      "  \n",
      "    \n",
      "  \n",
      "  Additional resources\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    LangGraph\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    GitHub\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Get started\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Get started\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Quickstarts\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Quickstarts\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Start with a prebuilt agent\n",
      "    \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Build a custom workflow\n",
      "\n",
      "Copyright © 2025 LangChain, Inc | Consent Preferences\n",
      "\n",
      "  \n",
      "  \n",
      "    Made with\n",
      "    \n",
      "      Material for MkDocs\n",
      "\n",
      "Because many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it's typically fairly quick to upgrade a sync graph to an async graph.\n",
      "See example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\n",
      "OpenAIAnthropicAzureGoogle GeminiAWS Bedrock\n",
      "\n",
      "\n",
      "pip install -U \"langchain[openai]\"\n",
      "\n",
      "import os\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
      "\n",
      "llm = init_chat_model(\"openai:gpt-4.1\")\n",
      "\n",
      "👉 Read the OpenAI integration docs\n",
      "\n",
      "\n",
      "pip install -U \"langchain[anthropic]\"\n",
      "\n",
      "import os\n",
      "from langchain.chat_models import init_chat_model\n",
      "\n",
      "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n",
      "\n",
      "llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n",
      "\n",
      "👉 Read the Anthropic integration docs\n",
      "\n",
      "\n",
      "pip install -U \"langchain[openai]\"\n",
      "\n",
      "import os\n",
      "from langchain.chat_models import init_chat_model\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "LangChain is a library that enables the creation of ambient agents. It provides a persistence layer called LangGraph, which supports features such as human-in-the-loop, memory, streaming, and deployment.\n"
     ]
    }
   ],
   "source": [
    "for msg in result['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b472fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Call Agent---------\n",
      "-------Check relevance--------\n",
      "--------Decision: Docs relevant--------\n",
      "--------Generate----------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is Langgraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever_vector_langgraph_blog (61s9e25m6)\n",
      " Call ID: 61s9e25m6\n",
      "  Args:\n",
      "    query: what is Langgraph?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_vector_langgraph_blog\n",
      "\n",
      "LangSmith Trace\n",
      "https://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\n",
      "What LangGraph provides¶\n",
      "By constructing each of the above in LangGraph, we get a few things:\n",
      "Persistence: Human-in-the-Loop¶\n",
      "LangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\n",
      "Persistence: Memory¶\n",
      "LangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\n",
      "Streaming¶\n",
      "LangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\n",
      "Deployment¶\n",
      "LangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Previous\n",
      "              \n",
      "\n",
      "                Run a local server\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Next\n",
      "              \n",
      "\n",
      "                Agent architectures\n",
      "\n",
      "Table of contents\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Learn LangGraph basics\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview¶\n",
      "LangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\n",
      "\n",
      "Reliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\n",
      "Low-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\n",
      "First-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\n",
      "\n",
      "Visualize your graph\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Mermaid\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      PNG\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to use the graph API¶\n",
      "This guide demonstrates the basics of LangGraph's Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph's control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\n",
      "Setup¶\n",
      "Install langgraph:\n",
      "pip install -U langgraph\n",
      "\n",
      "\n",
      "Set up LangSmith for better debugging\n",
      "Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\n",
      "\n",
      "Define and update state¶\n",
      "Here we show how to define and update state in LangGraph. We will demonstrate:\n",
      "\n",
      "Learn LangGraph basics¶\n",
      "To get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\n",
      "\n",
      "Build a basic chatbot\n",
      "Add tools\n",
      "Add memory\n",
      "Add human-in-the-loop controls\n",
      "Customize state\n",
      "Time travel\n",
      "\n",
      "In completing this series of tutorials, you will build a support chatbot in LangGraph that can:\n",
      "\n",
      "✅ Answer common questions by searching the web\n",
      "✅ Maintain conversation state across calls  \n",
      "✅ Route complex queries to a human for review  \n",
      "✅ Use custom state to control its behavior  \n",
      "✅ Rewind and explore alternative conversation paths  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Previous\n",
      "              \n",
      "\n",
      "                Start with a prebuilt agent\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Next\n",
      "              \n",
      "\n",
      "                1. Build a basic chatbot\n",
      "              \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Copyright © 2025 LangChain, Inc | Consent Preferences\n",
      "\n",
      "  \n",
      "  \n",
      "    Made with\n",
      "    \n",
      "      Material for MkDocs\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "LangGraph is a tool built for developers to create powerful, adaptable AI agents. It provides features such as persistence, streaming, and deployment for building custom agents with low-level primitives. LangGraph supports human-in-the-loop approvals and provides first-class streaming support.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke(\n",
    "    {'messages': \"what is Langgraph?\"}\n",
    ")\n",
    "for msg in result['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492229fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
